NOTE: THIS FILE HAS NOT BEEN UPDATED IN OVER FIVE YEARS.  THOUGH THE
INFORMATION IT CONTAINTS IS GENERALLY CORRECT FROM A HIGH LEVEL STANDPOINT,
IT IS NOT COMPLETE AND DETAILS MAY NOT BE CORRECT.

Written by:
  Kevin J. Bowers, Ph.D.
  Plasma Physics Group (X-1)
  Applied Physics Division
  Los Alamos National Lab
March/April 2004 - Original version

- Full 3D superhexahedral domain decomposition with arbitrary interdomain
  connectivity.

- Cartesian relativistic leap-frog particle advance with 6th order Boris
  rotation. Common case benchmarked at 7.8 million particle advances per second
  per processor on a Pentium 4 2.5GHz cluster (~1.9 Gflops/s ... approximately
  75% theoretical floating point performance).

- Leap frog Yee-mesh FDTD field solver with fully overlapped communications.
  99.85% field solve parallel fraction demonstrated on a Pentium 4 2.5GHz
  cluster with Gigabit ethernet. Most particle communications are overlapped as
  well.

- Exact (to machine precision) charge conserving current accumulation even in
  the presence of materials, particle reflection, particle absorption or
  passage of particles through multiple domains in a single step.

- "Energy conserving" field interpolation for particles.

- Periodic particle sorting to reduce cache thrashing (stabilized O(n) counting
  method).

- Exponentially differenced Ampere's law for support of diagonal tensor
  permittivity, permeability and conductive materials (even highly-conductive
  thin foils).

- Tunable short wavelength radition damping (modified transverse current
  method) for reduction of particle radiation noise and numerical Cherenkov.

- Symmetric, perfect magnetic conductor, anti-symmetric, perfect electric
  conductor and absorbing boundary conditions (second order accurate first
  order Higend with 15 degree annhiliation cone) for termination of domains.

- Electric and magnetic field divergence cleaning (modified Marder with optimal
  diffusion rate) to facilitate very long simulation runs (millions of
  timesteps).

- Time-centered energy, field, hydrodynamic and particle diagnostic dumps. The
  hydrodynamic dumps compute the density, the fluxes and the entire
  relativistically correct stress-energy tensor for a species (tri-linear node
  accumulation).

- "help" documented interface to MATLAB for simulation post-processing and
  visualization, including:
  - Reading raw domain grid, fields, hydrodynamic and particle dumps
  - Assembly of field, hydrodynamic and particle dumps from multiple domains
    for simple domain decompositions
  - Extraction of scalar and vector potentials (Coulomb gauge)
  - Average and spectral node-centering of Yee-mesh fields
  - Fourier smoothing of fields

- Checkpoint/restart capable.

- Support for large numbers of named species and materials (>billion and
  ~65K respectively).

- Performance monitoring of particle advance, particle communications, field
  solver and custom user routines.

- Extremely flexible compiler parsed input decks with support for custom
  diagnostics, particle/beam injection/emission, current injection, field/laser
  injection and internal structures. For example, regions and fields can be
  specified via equations:

set_region_material( x<0 && (x*x + y*y + z*z)<16, // Half sphere
                     "calcite",                   // Interior
                     "copper" );                  // Surface
set_region_field( everywhere,
                  0, 0, 0,
                  cos(theta)*B0*tanh(x/L), sin(theta)*B0*tanh(x/L), 0 )

- Unit agnostic implementation: Rationalized MKSA, Gaussian, Heaviside-Lorentz,
  natural ... can all be accomodated

- Portable support of SIMD 4-vector hardware acceleration provided through the
  V4 class library.

- High performance, high quality random number generation provided through the
  MTRAND library (support for many many random distributions, generator period
  of 2^19937-1, generator equidistribution through 623 dimensions).

- Written in ANSI-C/89, ANSI-C++/98 and MPI-1. The approximately 17K lines of
  strictly formatted and heavily commented code compiles with no warnings or
  errors, even at  "-Wall -pedantic -ansi".

- Strictly modular implementation enforced via modular compilation. Source code
  is organized hierarchically by module in a directory tree to facilitate
  maintenance and expansion.

- All MPI-1 calls are encapulated in the util/mp module to allow quick porting
  to other message passing libraries.

- Rapid porting provided through machine independent Makefile infrastructure.
  Though originally developed on x86/Linux clusters, porting to the Los Alamos
  Q machine (custom DEC Alpha supercomputer) consisted of writing a ~15 line
  machine description (took under an hour).

- Sample input decks including low-level test suite and benchmarks provided. 

